ML
  <一> 監督學習: 用於特定標籤的數據集
       1. 決策樹(Decision Trees)
          樹枝狀結構的是否問題評估正確決策的機率(機率樹)
          
          S -A -p(A) -P(BlA) ------>1              # 1+3=P(B) 
                     -P(B'lA) ----->2              # 2+4=P(B')
            -A'-P(A')-P(BlA') ----->3          
                     -P(B'lA')----->4
                    
    ***2. 貝氏分類(Bayesian classification)
          基於Bayes's rule與特徵間的獨立假設 : P(AlB)P(B) = P(BlA)P(A) 
          機率分類. 
          應用:垃圾郵件的判斷, 文章類型的分類, 判斷文字的情緒, 人臉辨識
          
       3. 最小平方法(Least Squares)
          線性回歸分析模型中, 求殘差平方和為最小的點估計量:求出一直線Y=a+bx+e,並對每一個點計算到直線的(垂直距離)^2,再相加 
                                                        => 最小化{e^2的相加總合}
    ***4. 邏輯回歸(Logistic Regression) : 線性-> 機率-> 類別
          將多個自變量X 來表示一個二項式結果
          透過邏輯函數來估計機率
          衡量依變量Y和多個自變量X間的關係 , 其中自變量X符合累計邏輯分布
          1) 線性模型: y = w * x + b   ==> 目標: 找合適的w和b , 使y所組成的線慢慢地接近所有的樣本點.
          2) 得到的分數y 標籤化: 
             a) Sigmoid函數: Sigmoid(x)=1/(1+exp(-x))
                適用於只對一個類別進行分類的情況
                將得到的分數y規範到(0,1)之間,函數值大於設定值則為1.
                以對數的方式對應到(0,1),強化分數大的作用,提高其輸出的機率. 降低分數小的輸出機率.
                def sigmoid(s):
                return 1. / (1+np.exp(-s))
                
             b) Softmax函數: softmax(s)=exp(s) / sum(exp(s)
                適用於對多個類別進行分類的情況
                將得到的分數規範到(0,1)之間,分數和為1.
                以對數的方式對應到(0,1),強化分數大的機率值, 弱化分數小的機率值
                
                x1 -- y1=wx+b -- sigmoid(s1)--           -- 機率p1 -- 類一        
                x2 -- y2=wx+b -- sigmoid(s2)--  Softmax  -- 機率p2 -- 類二
                x3 -- y3=wx+b -- sigmoid(s3)--           -- 機率p3 -- 類三
                
                def softmax(s):
                  return np.exp(s)/ np.sum(np.exp(s),axis=0)
          3) 評估預測的準確性
             目標: 衡量兩個機率分布向量的差異程度: D(y,p)在(0,1)之間,數值越高,兩個機率越相似, 表示模型預測越準確.
             p= 預測的機率向量 =softmax(score(wx+b))
             y= 真實的樣本向量
             D(y,p)= yln(p)+(1-y)(1-ln(p)) 
             Error= 1-D(y,p) 
             
             def cross_entropy(y,p):
              return np.sum(y*np.log(p)+(1-y)*np.log(1-p,axis=1))
              
         4) 訓練模型
            目標: 獲得更適合的 w 和 b, 使模型更準確/Error最低.
            方法: w & b => min(Sum(Error))
                  L(w,b)= (-1 / M )* (sum(D(y,p)))       
            def L(X,w,b,y):
                s=score(X,w,b)
                y_p=softmax(s)
                return -np.mean(cross_entropy(y,y_p))
           
            偏導數求x使y最小=> dy/dx = 0
            import numpy as np
            #假設函數y=1*x^2+0*x+0
            y=np.polyid([1,0,0])
            y(-7)  #輸出49 => 當x=-7時, y=49 
            #導函數d_yx => 看趨勢
            d_yx=np.polyder(y)
            d_yx(-7) # 輸出-14 => 當x=-7,y=49時, 導函數小於0 => x位於向下的趨勢
            
         5) 實現方式: 一個參數向量
            a)隨機選一個起點(x0,y0), 計算目的點的導數
              import random
              x0=random.uniform(-10,10)
              y0=random.uniform(-10,10)
              x0,y0
              
            b)依照導數的指引, 再往下一步走到新點(x1,y1)
              x1=x0-k0, 
              k0=alpha=走這一步的長度=學習率 
              => 一般學習率的參數不需要特別去注意, 有時特殊情況需要手動是要注意這會影響Gradient Descent的收斂速度及效果或者收斂失敗.
              => 學習率設定過小, 則收斂的速度會超乎想像的慢到生無可戀.
                 學習率設定過大, 則每一步的跨度太長, 則收斂失敗.
              
              def step(x,d_yx):
              alpha=.2
                  return x-alpha*d_yx(x)
              step(x0,d_yx)
              
            c)重複b)的步驟,進行收斂的動作,找到相對合適的參數.
              x=x0
              x_list=[x]
              for i in range(10):
              x=step(x,d_yx)
                 x_list.append(x)
              x_list
         5-1) 實現方式: 兩個參數向量
              b=b-alpha(y-yp)
            a)隨機選一個起點(w1,w2), 計算目的點的導數.
            b)依照導數的指引, 再往下一步走到新點(w1,w2).
            c)重複b)的步驟,進行收斂的動作,找到相對合適的參數.
            def loss_func(X,w,b,y,w_i):
                #損失函數對w的偏導數
                s=score(X,w,b)
                y_p=softmax(s)
                return np.mean(wi*(yp-y))
                
            def loss_func_b(X,w,b,y,d_obj=i):
                #損失函數對b的偏導數
                s=score(X,w,b)
                y_p=softmax(s)
                return np.mean(d_obj*(yp-y))
               
            def step(X,w,b,y,d_obj,loss_func):
                #alpha為學習率 /調節參數
                alpha=.2
                return wi-alpha*loss_func._call_(X,w,b,y,d_obj)
                
            class GDOptimizer:
              #Gradient Descent => 注意: 找到的參數組合僅是 "局部最佳" ,不代表是"全域最佳".
              def optimize(X,y):
                  w1=random.uniform(0,1)
                  w2=random.uniform(0,1)
                  b=random.uniform(0,1)
                  w=[w1,w2]
                  for i in range(100): #反覆運算100 times
                      w1=step(X,w,b,y,w1,loss_func)
                      w2=step(X,w,b,y,w2,loss_func)
                      b=step(X,w,b,y,b,loss_func_b)
                      w=[w1,w2]
         6) 使收斂速度快又穩定的方法: 設平均值=0  &  StandardScaler 歸一化(=>統一尺度)
            
            #兩個特徵向量
            f1=np.array([0.2,0.5,1.1]).reshape(-1,1)
            f2=np.array([-100.0,56.0,-77.0]).reshape(-1,1)
            
            #歸一化
            f1_scale=(f1-np.mean(f1))/np.std(f1)
            f2_scale=(f2-np.mean(f2))/np.std(f2)
            
            import sklearn.preprocessing as preprocessing
            scale=preprocessing.StandardScaler()
            f1_standard=scale.fit_transform(f1)
            f2_standard=scale.fit_transform(f2)
            
            assert np.allclose(f1_standard,f1_scale) and np.allclose(f2_standard,f2_scale)

          應用:信用評分, 計算事件的發生機率, 預測某產品的銷售額, 某天五月天會在我家開演唱會的機率(哈^^哈).
          
       5. 支持向量機(Support vector machine,SVM)
          邏輯回歸的進階版, 可將邏輯回歸得到的線優化,且還可以做非線性分類
          
       6. 集成方法(Ensemble)
          透過預測結果進行加權對新的數據點進行分類
          平均單個模型的偏差
          減少方差, 例如大盤的波動比單一股票的波動更少
          
  <二>無監督學習: 用於無標籤的數據集, 找出數據間的相關性
    ***1. 聚類算法(Clustering Algorithms)-K means
          將n個對象分成K個群組, 組內差異小,組間差異大     #組內變異=E(Var(XlY))  #組間變異=Var(E(XlY))  #組內變異+組間變異=總變異Var(X)
          
       2. 主成分分析(Principal component analysis,PCA)
          將一組可能存在相關性的變量觀測值轉換為一組線性不相關的變量值為主分量
          應用:簡化數據,可視化
          當成分有很高的方差則不適用PCA
          PCA為SVD的一種簡單應用: 最初的人臉辨識運用PCA及SVD將臉部特徵的線性組合降維,再透過方法匹配到身分.
          
       3. 獨立成分分析(Independent Component Analysis,ICA)
          數據變量由未知的變量線性混合,假設未知的變量非高斯分布且互相獨立,則這些未知的變量為獨立分量
          在同個場合中, N個人同時錄N個話,根據N個錄音文件恢復N個人的原始語音,ICA針對此盲源分離問題做分析
          找出數據變量的潛在因素
          應用:數字圖檔,文檔,指標
          
     **4. 奇異值分解(Singular value Decomposition,SVD)
          線性代數中一種重要的矩陣分解
          應用: 解決影像壓縮及去除雜訊問題,使影像資料有充分的解釋能力
          工具: R語言-library(rARRPACK)
          
   
